# -*- coding: utf-8 -*-
"""Airline passenger satisfaction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BYNwzYiu0nxRNpxwysZpV8Dth4iVUHPd
"""

!pip install pyspark

!pip install plotly==4.8.2



import pandas as pd
import numpy as np
import seaborn as sns
sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

import matplotlib.pyplot as plt

from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession, SQLContext

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import col

from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.classification import GBTClassifier

from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

spark = SparkSession.builder.appName("Customer Satisfaction").getOrCreate()
sc = spark.sparkContext
sqlContext = SQLContext(spark.sparkContext)

print(spark, sc, sqlContext)



from google.colab import drive
drive.mount('/content/drive')

"""Dataset import"""

test_data = '/content/drive/MyDrive/test.csv'
train_data = '/content/drive/MyDrive/train.csv'

"""Merging the dataset"""

df_test = spark.read.csv(path=test_data, inferSchema =True, header=True).cache()
df_train = spark.read.csv(path=train_data, inferSchema =True, header=True).cache()

#we merge both the test and train dataset
df_comp = df_test.union(df_train) 

# we check the summary
df_comp.summary().show()

"""Cleaning the dataset"""

#Dropping unwanted features from the dataset.
df_comp = df_comp.drop("_c0","id")



#Counting and replacing null values from Arrival delay column
df_comp.filter(col("Arrival Delay in Minutes").isNull()).count()
df_comp = df_comp.fillna({"Arrival Delay in Minutes":'15.1'})

print("Operation complete, count of null values: ",df_comp.filter(col("Arrival Delay in Minutes").isNull()).count())



#Blank characters are replaced with '_'
replace = {c:c.replace(' ','_') for c in df_comp.columns if ' ' in c}

#Replacing satisfied with '1'and neutral/dissatisfied with '0'
df_comp = df_comp.withColumn("satisfaction", F.when(F.col("satisfaction")=="satisfied", 1).otherwise(F.col("satisfaction")))
df_comp = df_comp.withColumn("satisfaction", F.when(F.col("satisfaction")=="neutral or dissatisfied", 0).otherwise(F.col("satisfaction")))

#satisfaction column type changed to Integer type
df_comp = df_comp.withColumn("satisfaction",col("satisfaction").cast(IntegerType()))

df_comp.dtypes

print("Number of rows: {}".format(df_comp.count()))
print("Number of columns: {}".format(len(df_comp.columns)))

"""Heatmap analysis"""

panda_df_comp = df_comp.toPandas()
plt.figure(figsize=(15,15))
sns.heatmap(abs(panda_df_comp.corr()), cmap = 'rocket_r', annot=True, fmt=".2f")



graph = plt.figure(figsize = (10,15))
plt.scatter(panda_df_comp['Departure Delay in Minutes'], panda_df_comp['Arrival Delay in Minutes'], alpha = 0.1)

plt.xlabel("Departure Delay in Minutes")
plt.ylabel("Arrival Delay in Minutes")

"""Since the arrival and Departure Delay are quite similar we can skip one of them by dropping them


"""

#Dropping Arrival delay

df_comp = df_comp.drop("Arrival Delay in Minutes")

"""DATA Visualization to check as to what affects satisfaction





"""

abs(panda_df_comp.corr()['satisfaction']).sort_values().drop('satisfaction').plot(kind='barh')

import plotly.express as px
graph1 = px.sunburst(panda_df_comp, path=["satisfaction",'Type of Travel','Class', 'Customer Type'],color_continuous_scale='deep')
graph1.show()

"""Age & Satisfaction


"""

age= sns.FacetGrid(panda_df_comp,col="satisfaction")
age.map(sns.histplot,"Age",bins=25)
plt.show()
# 0=neutral or dissatisfied, 1=satisfied

#Categorical attributes that were missing
panda_df_comp.hist(bins=50, figsize=(25,20))

"""Relation to Customer Type and thier Satisfaction


"""

ct=sns.catplot(x="Customer Type",y="satisfaction",data=panda_df_comp,kind="bar",height=7, palette="rocket")
ct.set_ylabels("Probability of Satisfaction")
plt.show()

"""Relation between type of travel and Satisfaction


"""

tt=sns.catplot(x="Class",y="satisfaction",data=panda_df_comp,kind="bar",height=7, palette="rocket")
tt.set_ylabels("Probability of satisfaction")
plt.show()

"""Data types manipulation"""

#To make things more simple and understandable we will Rename the satisfaction column to label
df_comp = df_comp.withColumnRenamed("satisfaction","label")

#Merge train and test dataframes into a single one for easier manipulation
train_df, test_df = df_comp.randomSplit([0.7, 0.3], seed=30)

print(f"Train set count: {train_df.count()} ")
print(f"Test set count: {test_df.count()} ")

#We now select only the categorical features and exclude the satisfaction label
c_cat = [x for (x, dataType) in df_comp.dtypes if ((dataType =="string") & (x !="label"))]
c_num = [x for (x, dataType) in df_comp.dtypes if ((dataType !="string") & (x !="label"))]

print(c_cat)
print(c_num)

"""TRAIN TEST SPLIT"""

#summary of all the types of data
train_df.dtypes

"""Transform categorical variables"""

from pyspark.ml.feature import (StringIndexer, OneHotEncoder)

string_indexer = [
    StringIndexer(inputCol=x, outputCol=x + "_StringIndexer", handleInvalid="skip")
    for x in c_cat
]
string_indexer

one_hot_encoder = [
    OneHotEncoder(
        inputCols=[f"{x}_StringIndexer" for x in c_cat],
        outputCols=[f"{x}_OneHotEncoder" for x in c_cat]
    )
]

one_hot_encoder

from pyspark.ml.feature import VectorAssembler

assemblerInput = [x for x in c_num]
assemblerInput += [f"{x}_OneHotEncoder" for x in c_cat]

assemblerInput

vector_assembler = VectorAssembler(
    inputCols = assemblerInput, outputCol="features"
)

vector_assembler

#from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression()

stages = []
stages += string_indexer
stages += one_hot_encoder
stages += [vector_assembler , lr] #Must be inserted as list

stages

#from pyspark.ml import Pipeline

pipeline = Pipeline().setStages(stages)
model_pp_lr = pipeline.fit(train_df)
predictions_pp_lr = model_pp_lr.transform(test_df)

predictions_pp_lr.select("features", "rawPrediction", "probability", "prediction","label").show()

"""AUC"""

#show
predictions_pp_lr.summary()

"""# ROC evaluation with Pyspark"""

#from pyspark.ml.evaluation import BinaryClassificationEvaluator

evaluator = BinaryClassificationEvaluator()
print('PySpark Area Under ROC', evaluator.evaluate(predictions_pp_lr))
print("Area Under PR: " + str(evaluator.evaluate(predictions_pp_lr, {evaluator.metricName: "areaUnderPR"})))

model_pp_lr.stages[-1].summary.pr.show()

#from pyspark.ml.evaluation import MulticlassClassificationEvaluator

print('Accuracy: ', MulticlassClassificationEvaluator(labelCol='label',metricName='accuracy').evaluate(predictions_pp_lr))
print('Precision: ',MulticlassClassificationEvaluator(labelCol='label',metricName='weightedPrecision').evaluate(predictions_pp_lr))
print('Recall: ',MulticlassClassificationEvaluator(labelCol='label',metricName='weightedRecall').evaluate(predictions_pp_lr))
print('f1: ',MulticlassClassificationEvaluator(labelCol='label',metricName='f1').evaluate(predictions_pp_lr))

trainingSummary = model_pp_lr.stages[-1].summary
roc = trainingSummary.roc.toPandas()
plt.plot(roc['FPR'],roc['TPR'])
plt.ylabel('False Positive Rate')
plt.xlabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
print('AreaUnderROC: ' + str(trainingSummary.areaUnderROC))

pr = trainingSummary.pr.toPandas()
plt.plot(pr['recall'],pr['precision'])
plt.ylabel('Precision')
plt.xlabel('Recall')
plt.title('PR Curve')
plt.show()

rmse = evaluator.evaluate(predictions_pp_lr)
print("RMSE: %g" % rmse)

#from pyspark.ml.classification import DecisionTreeClassifier

#dt = DecisionTreeClassifier(maxDepth = 3)

#stages = []
#stages += string_indexer
#stages += one_hot_encoder
#stages += [vector_assembler , dt] 

#stages

#pipeline = Pipeline().setStages(stages)
#model_pp_dt = pipeline.fit(train_df)
#predictions_pp_dt = model_pp_dt.transform(test_df)

#print("pipeline completed")

#evaluator = BinaryClassificationEvaluator()

#print("Area Under ROC: " + str(evaluator.evaluate(predictions_pp_dt, {evaluator.metricName: "areaUnderROC"})))
#print("Area Under PR: " + str(evaluator.evaluate(predictions_pp_dt, {evaluator.metricName: "areaUnderPR"})))

#print('Accuracy: ', MulticlassClassificationEvaluator(labelCol='label',metricName='accuracy').evaluate(predictions_pp_dt))
#print('Precision: ',MulticlassClassificationEvaluator(labelCol='label',metricName='weightedPrecision').evaluate(predictions_pp_dt))
#print('Recall: ',MulticlassClassificationEvaluator(labelCol='label',metricName='weightedRecall').evaluate(predictions_pp_dt))
#print('f1: ',MulticlassClassificationEvaluator(labelCol='label',metricName='f1').evaluate(predictions_pp_dt))

#rmse = evaluator.evaluate(predictions_pp_dt)
#print("RMSE: %g" % rmse)

"""Random Forest"""

from pyspark.ml.classification import RandomForestClassifier

rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')

stages = []
stages += string_indexer
stages += one_hot_encoder
stages += [vector_assembler , rf]

stages

pipeline = Pipeline().setStages(stages)
model_pp_rf = pipeline.fit(train_df)
predictions_pp_rf = model_pp_rf.transform(test_df)

print("pipeline has been set")

evaluator = BinaryClassificationEvaluator()

print("Area Under ROC: " + str(evaluator.evaluate(predictions_pp_rf, {evaluator.metricName: "areaUnderROC"})))
print("Area Under PR: " + str(evaluator.evaluate(predictions_pp_rf, {evaluator.metricName: "areaUnderPR"})))

print('Accuracy: ', MulticlassClassificationEvaluator(labelCol='label',metricName='accuracy').evaluate(predictions_pp_rf))
print('Precision: ',MulticlassClassificationEvaluator(labelCol='label',metricName='weightedPrecision').evaluate(predictions_pp_rf))
print('Recall: ',MulticlassClassificationEvaluator(labelCol='label',metricName='weightedRecall').evaluate(predictions_pp_rf))
print('f1: ',MulticlassClassificationEvaluator(labelCol='label',metricName='f1').evaluate(predictions_pp_rf))

trainingSummary = model_pp_rf.stages[-1].summary
roc = trainingSummary.roc.toPandas()
plt.plot(roc['FPR'],roc['TPR'])
plt.ylabel('False Positive')
plt.xlabel('True Positive')
plt.title('ROC Curve Graph')
plt.show()

print('AreaUnderROC: ' + str(trainingSummary.areaUnderROC))

pr = trainingSummary.pr.toPandas()
plt.plot(pr['recall'],pr['precision'])
plt.ylabel('Precision')
plt.xlabel('Recall')
plt.title('PR Curve')

plt.show()

rmse = evaluator.evaluate(predictions_pp_rf)
print("RMSE: %g" % rmse)